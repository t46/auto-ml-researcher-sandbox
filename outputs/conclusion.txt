We have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large
language models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach
in previous work that requires hand-crafting few-shot examples per task. Our simple method not only
is the minimalist and strongest zero-shot baseline for difficult multi-step system-2 reasoning tasks
that long evaded the scaling laws of LLMs, but also encourages the community to further discover
similar multi-task prompts that elicit broad cognitive abilities instead of narrow task-specific skills.