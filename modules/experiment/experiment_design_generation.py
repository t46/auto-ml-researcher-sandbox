# 実験計画の生成

from utils import get_llm_response, extract_blocks

def generate_experiment_design(problem, approach, approach_detail):
    prompt = '''
    Given the problem and accompanying approach below, generate an experiment design for a study that evaluates the effectiveness of the approach. 
    - Please provide a detailed verification plan composed of structured sentences. 
    - Ensure that the plan is sufficiently detailed and concrete so that it can be executed by a large language model and computer. 
    - Outline the procedure in a step-by-step manner. If necessary, break down a single task into multiple sub-tasks and list them hierarchically. 
    - The verification plan should be realistic and feasible, making use of existing resources rather than requiring the creation of new ones.
    - The expeiment design should be that is feasible to conduct autonomously by a computer.
    - The experiment design should include the evaluation metrics and the evaluation procedure.
    - The experiment design should be detailed and in structured format.
    - Output experiment design with <experiment> </experiment> tag.

    <problem>
        {problem}
    </problem>

    <approach>
        Overview:
            {approach}
        Detail:
            {approach_detail}
    </approach>
    '''

    prompt = prompt.format(problem=problem, approach=approach, approach_detail=approach_detail)

    response = get_llm_response(prompt)

    response = extract_blocks(response, r'<experiment>(.*?)</experiment>')

    return response

if __name__ == '__main__':
    problem = """
    Background:
    We use a Large Language Model (LLM), specifically GPT-4, which
    takes any text as input and outputs text in response. We input
    instructions, called prompts, to the LLM, and the LLM generates text
    based on those instructions.
    Problem:
    The issue is that the large language model may output sentences not
    directly related to the instructions.
    For example, if you enter the sentence "What is 1 + 1?" into the LLM,
    it will often respond with "The answer to that question is 2." In this
    response, what we really want is just the "2" part. The sentence "The
    answer to that question is" is extraneous, and we would prefer the
    LLM to output only the part that directly related to the question, "2".
    The reason this is problematic is that we must perform postprocessing to evaluate the output. For instance, if you want to
    evaluate the LLM's performance on a dataset of math problems, and
    a sample is a question "What is 1 + 1?" paired with the correct
    answer "2", we must check whether the LLM's answer matches "2". If
    the LLM outputs an extra sentence besides "2," even if the answer is
    actually correct, it may be judged as incorrect due to the apparent
    mismatch.
    It is challenging to address this issue with a predefined postprocessing method, as it is not known in advance what kind of
    extraneous text will be output.
    To sum up, the problems are as follows:
    - The large language model outputs sentences that are not directly
    related to the instructions.
    - Predefined post-processing methods are problem/answer-specific
    and not general.
    """
    approach = """
    Hypothesis: Refining the Prompting Strategy
    - We could refine the way we prompt the LLM. For instance, instead
    of asking "What is 1 + 1?", we could ask "Provide a one-word answer:
    What is 1 + 1?". This might encourage the model to generate more
    concise responses.
    """
    approach_detail = """
    Original Hypothesis:
    We could refine the way we prompt the LLM. For instance, instead of
    asking "What is 1 + 1?", we could ask "Provide a one-word answer:
    What is 1 + 1?". This might encourage the model to generate more
    concise responses.
    Refined Hypothesis:
    The refinement of the prompting strategy, specifically by requesting a
    one-word answer, will result in more concise responses from the
    Language Learning Model (LLM).
    To test this hypothesis, we can use a comparative analysis between
    the responses generated by the LLM when prompted with a standard
    question and when prompted with a one-word answer request.
    Mathematically, this can be represented as:
    Let's denote the standard prompt as P1 and the one-word answer
    prompt as P2. The responses generated by the LLM for these prompts
    are denoted as R1 and R2 respectively.
    The hypothesis can be tested by comparing the length (in words) of
    R1 and R2. If the length of R2 is less than or equal to the length of
    R1, the hypothesis is supported.
    Mathematically, this can be represented as:
    If Length(R2) ≤ Length(R1), then the hypothesis is supported.
    This hypothesis testing can be implemented using a computer
    program that prompts the LLM with P1 and P2, captures the
    responses R1 and R2, calculates their lengths, and compares them.
    """

    experiment_design = generate_experiment_design(problem, approach, approach_detail)
    print(experiment_design)